{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8c9bf4",
   "metadata": {},
   "source": [
    "### ctxt Packing\n",
    "Joonwoo Lee et al 2021 paces a channel of an image in a ctxt, \n",
    "using only 1024 slots out of 16384. (sparse packing)\n",
    "\n",
    "### Narrow Resnet\n",
    "Their 'ResNet-20' is a modified and simplified version of the original ResNet. \n",
    "First, they use only three basic blocks, while Resnet-18 consists of four. \n",
    "Second, they reduced the number of channels.  \n",
    "The first conv outputs 16 channels, instad of 64. \n",
    "The final number of channels is thus only 64. (Resnet-18 with four basic blocks ends up with 512 channels)\n",
    "This modification is reasonable because computations are done in per-channel manner. \n",
    "The total computing cost grows linearly with growing number of channels.  \n",
    "\n",
    "### AvgPool\n",
    "마지막에 AvgPool 하나와 FC가 하나 있음. 8x8 이미지를 8x8 kernel로 AvgPool해서 1 x 64-channel 이 됨. \n",
    "그 다음에 64개의 ctxt가 하나의 ctxt로 합쳐짐.  -- 어떻게 잘 합칠까? \n",
    "\n",
    "### Softmax\n",
    "Approximate softmax는 계산량이 상당히 많음. \n",
    "그러다고 softmax 없이 training을 할 수는 없음 (argmax는 differentiable하지 않으므로)\n",
    "1. softmax로 훈련한 뒤에 argmax로 교체해서 evaluate하거나 \n",
    "2.그 전에 decrypt해서 plain text에 softmax 계산하거나\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95de931",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d12902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d269571",
   "metadata": {},
   "source": [
    "Prepare Train / test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db2eedd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "num_workers = 0\n",
    "batch_size = 32\n",
    "valid_size = 0.2\n",
    "\n",
    "\n",
    "## Scale \n",
    "transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (2.5, 2.5, 2.5))\n",
    "     ])\n",
    "\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True,\n",
    "                              transform=transform\n",
    "                             )\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, \n",
    "                             transform=transform\n",
    "                            )\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7fd89",
   "metadata": {},
   "source": [
    "### 텐서 크기 \n",
    "1024 * 3  -> 1024 * 32 -> 256 * 32 -> 256 * 64 -> 64 * 64 -> 64 * 128 -> 16 * 128 -> 2048 -> 256 -> 10\n",
    "\n",
    "\n",
    "1024 * 32는 여러 개의 ctxt에 담아야함.  \n",
    "\n",
    "아니면 첫번째 convolution 채널을 작게 유지하고,   \n",
    "average pooling을 묶어서 계산할 수는 없을까.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ab9b3",
   "metadata": {},
   "source": [
    "## Polynomial fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bcdadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hemul.comparator import ApprRelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50bbed11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions set\n",
      "degrees = [15, 15, 15, 15], margin = 0.01, eps = 0.02\n"
     ]
    }
   ],
   "source": [
    "apr = ApprRelu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec28472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(arr):\n",
    "    _arr = arr.copy()\n",
    "    _arr[_arr<0]=0\n",
    "    return _arr\n",
    "\n",
    "xin = np.linspace(-0.999, 1.001, 100000)\n",
    "rans = relu(xin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d626fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = apr(xin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68125b19",
   "metadata": {},
   "source": [
    "### Approximate activation function\n",
    "\n",
    "function fitting은 Chebyshev로 하되, range와 deg를 바꿔가며 최적의 값을 찾을 것. \n",
    "* Multiplication detph는 log2(depth)이므로 (?) deg = 16 아니면 32를 쓰는게 좋을 듯.  -- 확인 필요\n",
    "\n",
    "* Validation test할 때 activation input의 min,max 범위 체크 필요 \n",
    "* min, max 범위가 제대로 커버 되었을 경우 Loss가 nan이 아닌 값이 나와야 함. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91f1c35d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet9(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (basicblock1): BasicBlock(\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (shortcut): Sequential()\n",
       "  )\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (basicblock2): BasicBlock(\n",
       "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (shortcut): Sequential()\n",
       "  )\n",
       "  (linear): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hemul import ResNet20 \n",
    "\n",
    "xx = np.linspace(-35, 28, 500)\n",
    "\n",
    "model = ResNet20.ResNet9(10)#, apr)#approx_relu8) # boundary 상관 없음\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8d38d",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2cad6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# For ResNet\n",
    "optimizer = optim.SGD(model.parameters(), lr=.01, weight_decay=5e-4, momentum = 0.9)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=.001, weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585db7f0",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "activation을 approximate relu로 설정하면 loss = nan 나옴. \n",
    "1. input range 체크 \n",
    "2. 중간값 범위 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de6acf62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.176866 \tValidation Loss: 0.212510\n",
      "Validation loss decreased (inf --> 0.212510).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.754147 \tValidation Loss: 0.179473\n",
      "Validation loss decreased (0.212510 --> 0.179473).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.586185 \tValidation Loss: 0.151994\n",
      "Validation loss decreased (0.179473 --> 0.151994).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.482851 \tValidation Loss: 0.139160\n",
      "Validation loss decreased (0.151994 --> 0.139160).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.405050 \tValidation Loss: 0.110983\n",
      "Validation loss decreased (0.139160 --> 0.110983).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.346083 \tValidation Loss: 0.124491\n",
      "Epoch: 7 \tTraining Loss: 0.293177 \tValidation Loss: 0.122744\n",
      "Epoch: 8 \tTraining Loss: 0.252588 \tValidation Loss: 0.100088\n",
      "Validation loss decreased (0.110983 --> 0.100088).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.213902 \tValidation Loss: 0.100147\n",
      "Epoch: 10 \tTraining Loss: 0.181604 \tValidation Loss: 0.108991\n",
      "Epoch: 11 \tTraining Loss: 0.153793 \tValidation Loss: 0.131980\n",
      "Epoch: 12 \tTraining Loss: 0.134921 \tValidation Loss: 0.106523\n",
      "Epoch: 13 \tTraining Loss: 0.111521 \tValidation Loss: 0.118029\n",
      "Epoch: 14 \tTraining Loss: 0.097345 \tValidation Loss: 0.131111\n",
      "Epoch: 15 \tTraining Loss: 0.094058 \tValidation Loss: 0.125132\n",
      "Epoch: 16 \tTraining Loss: 0.082485 \tValidation Loss: 0.125441\n",
      "Epoch: 17 \tTraining Loss: 0.072829 \tValidation Loss: 0.167139\n",
      "Epoch: 18 \tTraining Loss: 0.071952 \tValidation Loss: 0.139847\n",
      "Epoch: 19 \tTraining Loss: 0.062204 \tValidation Loss: 0.122335\n",
      "Epoch: 20 \tTraining Loss: 0.070305 \tValidation Loss: 0.145335\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "train_losslist=[]\n",
    "valid_loss_min = np.Inf \n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        #print(\"DATA\", data[0,0,0])\n",
    "        output = model(data)\n",
    "        #print(\"weight\", model.conv1.weight[0,0,0])\n",
    "        #print(output[0,:])\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #print(\"VALID-------------------------------------------------------------------\")\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    train_losslist.append(train_loss)\n",
    "        \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'ResNet9avg_minimaxReLU.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "864bd917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('ResNet9avg_minimaxReLU.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99fe074",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70b8f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.739771\n",
      "\n",
      "Test Accuracy of airplane: 89% (898/1000)\n",
      "Test Accuracy of automobile: 88% (886/1000)\n",
      "Test Accuracy of  bird: 73% (731/1000)\n",
      "Test Accuracy of   cat: 73% (730/1000)\n",
      "Test Accuracy of  deer: 81% (810/1000)\n",
      "Test Accuracy of   dog: 74% (744/1000)\n",
      "Test Accuracy of  frog: 79% (796/1000)\n",
      "Test Accuracy of horse: 91% (914/1000)\n",
      "Test Accuracy of  ship: 80% (806/1000)\n",
      "Test Accuracy of truck: 86% (861/1000)\n",
      "\n",
      "Test Accuracy (Overall): 81% (8176/10000)\n"
     ]
    }
   ],
   "source": [
    "# track test loss\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    #### VALIDATE \n",
    "    output = model.validate(data)\n",
    "    #model.minmax=[torch.inf, -torch.inf]\n",
    "    ###    \n",
    "\n",
    "    loss = criterion(output, target)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "\n",
    "#torch.save(model.state_dict(), \"Net_3C2F.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56488a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83becbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04042d3d",
   "metadata": {},
   "source": [
    "\n",
    "xx = np.linspace(-33, 26, 1000)\n",
    "activation = Chebyshev.fit(xx, ELU(xx), deg=31)\n",
    "\n",
    "\n",
    "Test Loss: 0.951408\n",
    "\n",
    "Test Accuracy of airplane: 77% (778/1000)\n",
    "Test Accuracy of automobile: 83% (838/1000)\n",
    "Test Accuracy of  bird: 66% (666/1000)\n",
    "Test Accuracy of   cat: 73% (734/1000)\n",
    "Test Accuracy of  deer: 72% (727/1000)\n",
    "Test Accuracy of   dog: 55% (553/1000)\n",
    "Test Accuracy of  frog: 57% (577/1000)\n",
    "Test Accuracy of horse: 77% (771/1000)\n",
    "Test Accuracy of  ship: 33% (335/1000)\n",
    "Test Accuracy of truck: 76% (760/1000)\n",
    "\n",
    "Test Accuracy (Overall): 67% (6739/10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37649b5e",
   "metadata": {},
   "source": [
    "xx = np.linspace(-35, 28, 100)\n",
    "activation = Chebyshev.fit(xx, ELU(xx), deg=63)\n",
    "\n",
    "\n",
    "\n",
    "Test Loss: 0.616481\n",
    "\n",
    "Test Accuracy of airplane: 79% (791/1000)\n",
    "Test Accuracy of automobile: 89% (899/1000)\n",
    "Test Accuracy of  bird: 69% (691/1000)\n",
    "Test Accuracy of   cat: 71% (715/1000)\n",
    "Test Accuracy of  deer: 81% (815/1000)\n",
    "Test Accuracy of   dog: 67% (674/1000)\n",
    "Test Accuracy of  frog: 86% (864/1000)\n",
    "Test Accuracy of horse: 81% (814/1000)\n",
    "Test Accuracy of  ship: 84% (848/1000)\n",
    "Test Accuracy of truck: 86% (860/1000)\n",
    "\n",
    "Test Accuracy (Overall): 79% (7971/10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
